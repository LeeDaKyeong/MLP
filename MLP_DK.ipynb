{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class activation:\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    def relu(self, x):\n",
    "        x[x<0] = 0 \n",
    "        return x\n",
    "    def elu(self, x):\n",
    "        return   \n",
    "    def softmax(self, x):\n",
    "        #return np.exp(x)/sum(np.exp(x))\n",
    "        return np.exp(x) / np.sum(np.exp(x),axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class optimizer:\n",
    "    def loss(self, y_pred, y, loss_type = \"mse\"):\n",
    "        if loss_type.lower() == \"mse\":\n",
    "            return np.sum(np.square(y_pred-y))\n",
    "        elif loss_type.lower() == \"cross_entropy\": # softmax\n",
    "            return np.sum(-y*(np.log(activation().softmax(y_pred))))\n",
    "    \n",
    "    def SGD(self, learning_rate, activate_type, weight_dic, y):\n",
    "        if len(weight_dic) == 2: # softmaxë§Œ\n",
    "            weight_dic[1] = -weight_dic[0]*(1-y)\n",
    "            return weight_dic\n",
    "        else:\n",
    "            #weight_dic[len(weight_dic)] = -weight_dic[0]*(1-y)\n",
    "            \n",
    "            if activate_type.lower() == \"sigmoid\":\n",
    "                return weight_dic\n",
    "            elif activate_type.lower() == \"relu\":\n",
    "                return weight_dic\n",
    "    \n",
    "    def accuracy(self, y_pred, y): # y_pred,y : 60000,10\n",
    "        cnt = 0\n",
    "        y_pred = np.argmax(y_pred,axis=1) # 60000\n",
    "        y = np.argmax(y,axis=1)\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i]==y[i]:\n",
    "                cnt += 1\n",
    "        return cnt/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def weight(self, shape=[None,None]):\n",
    "        return np.random.normal(size = shape) / np.sqrt(shape[0]/2)\n",
    "    def bias(self, shape=[None]):\n",
    "        return np.random.normal(size = shape) / np.sqrt(shape[0]/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/apple/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n",
      "(60000,) (10000,)\n",
      "(60000, 10) (10000, 10)\n",
      "(60000, 784) (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "((X_train, y_train), (X_test, y_test)) = mnist.load_data()\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "X_train = X_train.reshape(60000, 28*28)\n",
    "X_test = X_test.reshape(10000, 28*28)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = layer().weight(shape = [784, 256])\n",
    "W2 = layer().weight(shape = [256, 256])\n",
    "W3 = layer().weight(shape = [784, 10])\n",
    "\n",
    "weight_dic = {0:X_train, 1:W1, 2:W2, 3:W3}\n",
    "\n",
    "for i in range(training_epochs):\n",
    "    L1 = activation().sigmoid(X_train.dot(weight_dic['W1']))\n",
    "    L2 = activation().sigmoid(L1.dot(weight_dic['W2']))\n",
    "    hypothesis = L2.dot(weight_dic['W3'])\n",
    "    \n",
    "    cost = optimizer.loss(hypothesis,y_train,loss_type=\"cross_entropy\")\n",
    "    weight_dic = optimizer().SGD(learning_rate=learning_rate, activate_type='sigmoid', weight_dic, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
